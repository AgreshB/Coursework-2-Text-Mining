{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoEncWHm5s28"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1V7EzFr6N7p"
   },
   "source": [
    "## Library installation/import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1B0__6AYci7"
   },
   "source": [
    "Install and import libraries that are used in multiple sections of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6K2gnqqfRsoS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s7ivGrSR5NaA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (3.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (4.55.1)\n",
      "Requirement already satisfied: setuptools in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (51.0.0.post20201207)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (0.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (3.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (2.22.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (3.0.5)\n",
      "Requirement already satisfied: jinja2 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (2.11.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (8.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (3.7.4.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (0.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (2.4.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (1.7.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (1.17.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (20.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (2.0.1)\n",
      "Requirement already satisfied: pathy in /Users/Radha/anaconda3/lib/python3.6/site-packages (from spacy==3.0) (0.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from importlib-metadata>=0.20->spacy==3.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from packaging>=20.0->spacy==3.0) (2.4.7)\n",
      "Requirement already satisfied: dataclasses>=0.6 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from pydantic<1.8.0,>=1.7.1->spacy==3.0) (0.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (2019.9.11)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.0->spacy==3.0) (2.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.0->spacy==3.0) (0.14)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from typer<0.4.0,>=0.3.0->spacy==3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from jinja2->spacy==3.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from pathy->spacy==3.0) (3.0.0)\n",
      "Requirement already satisfied: tweepy in /Users/Radha/anaconda3/lib/python3.6/site-packages (3.10.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from tweepy) (1.13.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: wordcloud in /Users/Radha/anaconda3/lib/python3.6/site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in /Users/Radha/anaconda3/lib/python3.6/site-packages (from wordcloud) (3.3.3)\n",
      "Requirement already satisfied: pillow in /Users/Radha/anaconda3/lib/python3.6/site-packages (from wordcloud) (8.1.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from wordcloud) (1.17.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.7.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Radha/anaconda3/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: six in /Users/Radha/anaconda3/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==3.0\n",
    "!pip install tweepy\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_S1J4oc6laA"
   },
   "source": [
    "## Configuring Twitter API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_k-DQXnYXyIO"
   },
   "source": [
    "Please note that the API keys below are the course leader's own API keys. You are allowed to use it to do some small tests, but please be careful because all students in the class now have a copy of it, and hence the limits can be easily exceeded.\n",
    "\n",
    "If your group has decided to use Twitter data, you can [apply for your own keys](https://developer.twitter.com/en/apply-for-access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0v0dZ8fCnqI"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler('fjkruboMzTLE4BLE7FmEpkWpw', 'jDobYz45Ksc3uMHoD2QnyZK60NwfRZnWIDVmyPtUGLkiOUqfGl')\n",
    "auth.set_access_token('1374773661861830658-lPZKU2qeuepRxVfWs5OxRoZd6XGzrH', '84k6xIDIMrt5mzPFLCoesD0WM9bpk8d3bAKaNonbcuT0s')\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sBB2guxczux"
   },
   "source": [
    "## Downloading of new data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5EwgFk86EfS"
   },
   "source": [
    "**IMPORTANT NOTE**: Please do not run the cell below unless intending to download a new data set.\n",
    "\n",
    "Make sure that you change the parameters.\n",
    "\n",
    "Also, check the [Tweepy API reference](https://docs.tweepy.org/en/latest/api.html) to find out about other ways through which you can retrieve tweets, e.g., by specifying usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSQXfQVGCpmj"
   },
   "outputs": [],
   "source": [
    "# Collect tweets\n",
    "query = \"#notoracism\" + \" -filter:retweets\"\n",
    "cutoff_date = \"2021-01-01\"\n",
    "tweets = tweepy.Cursor(api.search, q=query, lang=\"en\", since=cutoff_date).items(1000)\n",
    "\n",
    "tweets_list = [[tweet.created_at, tweet.user.screen_name, tweet.user.location, tweet.text] for tweet in tweets]\n",
    "\n",
    "tweets_df = pd.DataFrame(data=tweets_list, columns=['date', 'user', 'location', 'text'])\n",
    "\n",
    "# A good idea to save downloaded tweets as CSV\n",
    "tweets_df.to_csv ('current_set.csv', quotechar='\"', encoding='utf8', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qumfu0NwYKqR"
   },
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giyVyGPAYUTK"
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K23I1qBNZ_HQ"
   },
   "source": [
    "Below we provide some code for text cleaning. However, we encourage you to think of other ways to clean your data, e.g., by removing hashtags, removing usernames, removing duplicate tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /Users/Radha/anaconda3/lib/python3.6/site-packages (0.6.0)\n",
      "Requirement already satisfied: wordsegment in /Users/Radha/anaconda3/lib/python3.6/site-packages (1.3.1)\n",
      "Requirement already satisfied: autocorrect in /Users/Radha/anaconda3/lib/python3.6/site-packages (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "#installing tweet-preprocessor\n",
    "!pip install tweet-preprocessor\n",
    "!pip install wordsegment\n",
    "!pip install autocorrect\n",
    "import preprocessor as p\n",
    "from wordsegment import load, segment\n",
    "import re\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment if not using pre saved data set \n",
    "tweets_df = pd.read_csv('current_set.csv', quotechar='\"', encoding='utf8')\n",
    "cleaned_tweets = []\n",
    "\n",
    "# using the tweet-processor library to clean the tweets \n",
    "# library removes all links and mentions and hastags\n",
    "\n",
    "# include hastags and remove urls and emojis and mentions\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.MENTION)\n",
    "for tweet in tweets_df['text']:\n",
    "    cleaned_tweets.append(p.clean(tweet))\n",
    "\n",
    "# adding back to data frame\n",
    "tweets_df['text_processed'] = cleaned_tweets\n",
    "\n",
    "#removing punctuation\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: re.sub('[,\\\\.!?]', '', x))\n",
    "\n",
    "# Remove unnecessary line breaks\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: re.sub(r\"\\n\", '', x))\n",
    "\n",
    "# loading the segmenter to deal with hastags\n",
    "# hastags are just a multiple words joined together\n",
    "# we thus seperate the words and join it to the tweet\n",
    "load()\n",
    "#store hastags in different column \n",
    "tweets_df['hashtag'] =  tweets_df['text_processed'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) #creating a new column\n",
    "# remove hastag from tweet\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: re.sub(r\"#(\\w+)\", '', x))\n",
    "\n",
    "# ---- segmentation of hastag ----\n",
    "# joining this to the original tweet\n",
    "for index , tag in enumerate(tweets_df['hashtag']):\n",
    "    segmented_hashtag = ' '.join(segment(' '.join(tag)))\n",
    "    tweets_df['text_processed'][index] = tweets_df['text_processed'][index] + \" \" + segmented_hashtag                  \n",
    "# ------\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "#removing extra whitespaces \n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: ' '.join(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-06 10:35:34</td>\n",
       "      <td>RWDMolenbeek</td>\n",
       "      <td>Sint-Jans-Molenbeek, België</td>\n",
       "      <td>🏳️‍🌈Matchday: R.W.D.M - @KMSKDeinze 🏟E. Machte...</td>\n",
       "      <td>matchday: rwdm - e machtens stadion 20:45 no t...</td>\n",
       "      <td>[#NOtoRacism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-06 10:24:42</td>\n",
       "      <td>motsetse_sello</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I've never seen I a white Human ever since I w...</td>\n",
       "      <td>i've never seen i a white human ever since i w...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-06 07:24:48</td>\n",
       "      <td>amirjon4628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi army,This is Iranian armys\\nWe just found m...</td>\n",
       "      <td>hi armythis is iranian armys we just found meh...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-06 07:12:20</td>\n",
       "      <td>future_nostalgi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi army,This is Iranian armys\\nWe just found m...</td>\n",
       "      <td>hi armythis is iranian armys we just found meh...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-06 07:09:39</td>\n",
       "      <td>NJ7twt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Nili10724948 @MehradHidden Whenever the hater...</td>\n",
       "      <td>whenever the hater starts to hate bts does not...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date             user                     location  \\\n",
       "0  2021-03-06 10:35:34     RWDMolenbeek  Sint-Jans-Molenbeek, België   \n",
       "1  2021-03-06 10:24:42   motsetse_sello                          NaN   \n",
       "2  2021-03-06 07:24:48      amirjon4628                          NaN   \n",
       "3  2021-03-06 07:12:20  future_nostalgi                          NaN   \n",
       "4  2021-03-06 07:09:39           NJ7twt                          NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  🏳️‍🌈Matchday: R.W.D.M - @KMSKDeinze 🏟E. Machte...   \n",
       "1  I've never seen I a white Human ever since I w...   \n",
       "2  Hi army,This is Iranian armys\\nWe just found m...   \n",
       "3  Hi army,This is Iranian armys\\nWe just found m...   \n",
       "4  @Nili10724948 @MehradHidden Whenever the hater...   \n",
       "\n",
       "                                      text_processed        hashtag  \n",
       "0  matchday: rwdm - e machtens stadion 20:45 no t...  [#NOtoRacism]  \n",
       "1  i've never seen i a white human ever since i w...             []  \n",
       "2  hi armythis is iranian armys we just found meh...             []  \n",
       "3  hi armythis is iranian armys we just found meh...             []  \n",
       "4  whenever the hater starts to hate bts does not...             []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tweets_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22qj0jq3YklZ"
   },
   "source": [
    "## Exploration using a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IIh30BjaWji"
   },
   "source": [
    "Generating a word cloud is one way by which you can check whether your data needs any further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqxuMbjk8EkA"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# join the words of the different tweets together into one string\n",
    "long_string = ' '.join(unique_tweets)\n",
    "new_long_string = ' '.join(set(long_string.split(\" \")))\n",
    "\n",
    "# create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# generate a word cloud\n",
    "wordcloud.generate(new_long_string)\n",
    "\n",
    "# visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQyNo3FYYuED"
   },
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwAZb_l-7C-L"
   },
   "outputs": [],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you want to modify this by adding more stop words?\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words and Clean-up textn-up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = tweets_df.text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stopwords and Lemmatize\n",
    "\n",
    "The advantage of Lemmatization is that it can reduce the total number of unique words in the dictionary.The ultimate goal of lemmatization is to help the LDA model to produce better topics in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "#Define functions for Lemmatization\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = [ ]\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Do lemmatization, keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# convert the corpus into a BoW representation\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use model perplexity and topic coherence to decide the number of topics.\n",
    "model_list = []\n",
    "perplexity = []\n",
    "coherence_values = []\n",
    "\n",
    "for num_topics in range(2,21,1):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                               id2word=id2word,\n",
    "                               random_state=1,\n",
    "                               num_topics=num_topics)\n",
    "    model_list.append(lda_model)\n",
    "    \n",
    "#Calculate perplexity\n",
    "    perplexity_values = lda_model.log_perplexity(corpus)\n",
    "    print('Perplexity of %d topics is: ' % (num_topics-1), perplexity_values) # a measure of how good the model is. lower the better.\n",
    "    perplexity.append(perplexity_values)\n",
    "\n",
    "#Calculate coherence\n",
    "    coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    coherence_values.append(round(coherencemodel.get_coherence(),3))\n",
    "    print('The Coherence of %d topics is: ' % (num_topics-1), round(coherencemodel.get_coherence(),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Draw Perplexity-Coherence-Topic line chart\n",
    "#Use the subplot() method to draw multiple graphs\n",
    "plt.figure(figsize=(16,5),dpi=200)\n",
    "x = range(2,21,1)\n",
    "\n",
    "#The drawing board is divided into blocks composed of 2 rows and 1 column, and the first area is obtained\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "#Draw in the first subarea\n",
    "plt.plot(x,perplexity)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Perplexity score\")\n",
    "plt.xticks(range(1,21,2))#Set the scale\n",
    "plt.title('Perplexity')\n",
    "plt.grid(True, alpha=0.5)\n",
    "\n",
    "#Select the second sub-area and draw\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "plt.plot(x,coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.xticks(range(1,21,2))#Set the scale\n",
    "plt.title('Coherence')\n",
    "plt.grid(True, alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# set number of topics\n",
    "num_topics = 12\n",
    "\n",
    "# build an LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "\n",
    "# print keywords in each topic\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the topics\n",
    "!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./'+str(num_topics))\n",
    "\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "  pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "  LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7UoGTRZY1pg"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3RLsS3Ac8v7"
   },
   "source": [
    "This implementation is based on the lexicon- and rule-based [VADER](https://github.com/cjhutto/vaderSentiment) sentiment analysis tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gas4oUUIY8iF"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hK8WbOHbA78"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for tweet_text in unique_tweets:\n",
    "    vs = analyzer.polarity_scores(tweet_text)\n",
    "    print(tweet_text + '\\t' + str(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT11HUQBY47n"
   },
   "source": [
    "# Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcanVP2udd24"
   },
   "source": [
    "This implementation is based on [spaCy's model](https://spacy.io/models/en#en_core_web_trf) using contextualised embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUWgWiLZYaBS"
   },
   "outputs": [],
   "source": [
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_core_web_trf\n",
    "import spacy\n",
    "import en_core_web_trf\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAvsa90Gpwna"
   },
   "outputs": [],
   "source": [
    "for tweet_text in unique_tweets:\n",
    "  doc = nlp(tweet_text)\n",
    "  print(tweet_text)\n",
    "  for ne in doc.ents:\n",
    "    print('\\tNE found: ', ne.start_char, ne.end_char, ne.label_, tweet_text[ne.start_char:ne.end_char])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4Ub0m8zoZfH"
   },
   "source": [
    "# Named Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqLYLoM7ecLP"
   },
   "source": [
    "This implementation is based on [spaCy Entity Linker](https://github.com/egerber/spacy-entity-linker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvKRih-dYEDL"
   },
   "outputs": [],
   "source": [
    "!pip install spacy-entity-linker\n",
    "!python -m spacyEntityLinker \"download_knowledge_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7bxxYpFcuoz"
   },
   "outputs": [],
   "source": [
    "from spacyEntityLinker import EntityLinker\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory(\n",
    "   \"entityLinker\"\n",
    ")\n",
    "def create_linker(nlp, name):\n",
    "  return EntityLinker()\n",
    "\n",
    "#add to pipeline\n",
    "nlp.add_pipe('entityLinker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnZZ0t32oCgZ"
   },
   "outputs": [],
   "source": [
    "for tweet_text in unique_tweets:\n",
    "  doc = nlp(tweet_text)\n",
    "  print(tweet_text)\n",
    "  all_linked_entities = doc._.linkedEntities\n",
    "  all_linked_entities.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Coursework 2: SMA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
